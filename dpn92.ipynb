{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7599917,"sourceType":"datasetVersion","datasetId":4424072},{"sourceId":7845062,"sourceType":"datasetVersion","datasetId":4599777},{"sourceId":7836302,"sourceType":"datasetVersion","datasetId":4593384}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\nDual Path Networks\nCombines ResNeXt grouped convolutions and DenseNet dense\nconnections to acheive state-of-the-art performance on ImageNet\n\nReferences:\n    - [Dual Path Networks](https://arxiv.org/abs/1707.01629)\n'''\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\nimport os\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n\n\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Lambda\nfrom keras.layers import Activation\nfrom keras.layers import BatchNormalization\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers import Conv2D\nfrom keras.layers import concatenate\nfrom keras.layers import add\nfrom keras.regularizers import l2\nfrom tensorflow.python.keras.utils import conv_utils\nfrom keras_applications.imagenet_utils import _obtain_input_shape\nfrom keras.applications.imagenet_utils import decode_predictions\nfrom keras import backend as K\n\n__all__ = ['DualPathNetwork', 'DPN92', 'DPN98', 'DPN137', 'DPN107', 'preprocess_input', 'decode_predictions']\n\n\ndef preprocess_input(x, data_format=None):\n    \"\"\"Preprocesses a tensor encoding a batch of images.\n       Obtained from https://github.com/cypw/DPNs\n\n        # Arguments\n            x: input Numpy tensor, 4D.\n            data_format: data format of the image tensor.\n\n        # Returns\n            Preprocessed tensor.\n        \"\"\"\n    if data_format is None:\n        data_format = K.image_data_format()\n    assert data_format in {'channels_last', 'channels_first'}\n\n    if data_format == 'channels_first':\n        # 'RGB'->'BGR'\n        x = x[:, ::-1, :, :]\n        # Zero-center by mean pixel\n        x[:, 0, :, :] -= 104\n        x[:, 1, :, :] -= 117\n        x[:, 2, :, :] -= 128\n    else:\n        # 'RGB'->'BGR'\n        x = x[:, :, :, ::-1]\n        # Zero-center by mean pixel\n        x[:, :, :, 0] -= 104\n        x[:, :, :, 1] -= 117\n        x[:, :, :, 2] -= 124\n\n    x *= 0.0167\n    return x\n\n\ndef DualPathNetwork(input_shape=None,\n                    initial_conv_filters=64,\n                    depth=[3, 4, 20, 3],\n                    filter_increment=[16, 32, 24, 128],\n                    cardinality=32,\n                    width=3,\n                    weight_decay=0,\n                    include_top=True,\n                    weights=None,\n                    input_tensor=None,\n                    pooling=None,\n                    classes=1000):\n    \"\"\" Instantiate the Dual Path Network architecture for the ImageNet dataset. Note that ,\n        when using TensorFlow for best performance you should set\n        `image_data_format=\"channels_last\"` in your Keras config\n        at ~/.keras/keras.json.\n        The model are compatible with both\n        TensorFlow and Theano. The dimension ordering\n        convention used by the model is the one\n        specified in your Keras config file.\n        # Arguments\n            initial_conv_filters: number of features for the initial convolution\n            depth: number or layers in the each block, defined as a list.\n                DPN-92  = [3, 4, 20, 3]\n                DPN-98  = [3, 6, 20, 3]\n                DPN-131 = [4, 8, 28, 3]\n                DPN-107 = [4, 8, 20, 3]\n            filter_increment: number of filters incremented per block, defined as a list.\n                DPN-92  = [16, 32, 24, 128]\n                DON-98  = [16, 32, 32, 128]\n                DPN-131 = [16, 32, 32, 128]\n                DPN-107 = [20, 64, 64, 128]\n            cardinality: the size of the set of transformations\n            width: width multiplier for the network\n            weight_decay: weight decay (l2 norm)\n            include_top: whether to include the fully-connected\n                layer at the top of the network.\n            weights: `None` (random initialization) or `imagenet` (trained\n                on ImageNet)\n            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n                to use as image input for the model.\n            input_shape: optional shape tuple, only to be specified\n                if `include_top` is False (otherwise the input shape\n                has to be `(224, 224, 3)` (with `tf` dim ordering)\n                or `(3, 224, 224)` (with `th` dim ordering).\n                It should have exactly 3 inputs channels,\n                and width and height should be no smaller than 8.\n                E.g. `(200, 200, 3)` would be one valid value.\n            pooling: Optional pooling mode for feature extraction\n                when `include_top` is `False`.\n                - `None` means that the output of the model will be\n                    the 4D tensor output of the\n                    last convolutional layer.\n                - `avg` means that global average pooling\n                    will be applied to the output of the\n                    last convolutional layer, and thus\n                    the output of the model will be a 2D tensor.\n                - `max` means that global max pooling will\n                    be applied.\n                - `max-avg` means that both global average and global max\n                    pooling will be applied to the output of the last\n                    convolution layer\n            classes: optional number of classes to classify images\n                into, only to be specified if `include_top` is True, and\n                if no `weights` argument is specified.\n        # Returns\n            A Keras model instance.\n        \"\"\"\n\n    if weights not in {'imagenet', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `imagenet` '\n                         '(pre-training on ImageNet).')\n\n    if weights == 'imagenet' and include_top and classes != 1000:\n        raise ValueError('If using `weights` as imagenet with `include_top`'\n                         ' as true, `classes` should be 1000')\n\n    assert len(depth) == len(filter_increment), \"The length of filter increment list must match the length \" \\\n                                                \"of the depth list.\"\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=112,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    x = _create_dpn(classes, img_input, include_top, initial_conv_filters,\n                    filter_increment, depth, cardinality, width, weight_decay, pooling)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = tf.keras.utils.get_source_inputs(input_tensor, layer=None, node_index=None)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name='resnext')\n\n    # load weights\n\n    return model\n\n\ndef DPN92(input_shape=None,\n          include_top=True,\n          weights=None,\n          input_tensor=None,\n          pooling=None,\n          classes=13):\n    return DualPathNetwork(input_shape, include_top=include_top, weights=weights, input_tensor=input_tensor,\n                           pooling=pooling, classes=classes)\n\n\ndef DPN98(input_shape=None,\n          include_top=True,\n          weights=None,\n          input_tensor=None,\n          pooling=None,\n          classes=13):\n    return DualPathNetwork(input_shape, initial_conv_filters=96, depth=[3, 6, 20, 3], filter_increment=[16, 32, 32, 128],\n                           cardinality=40, width=4, include_top=include_top, weights=weights, input_tensor=input_tensor,\n                           pooling=pooling, classes=classes)\n\n\ndef DPN137(input_shape=None,\n           include_top=True,\n           weights=None,\n           input_tensor=None,\n           pooling=None,\n           classes=13):\n    return DualPathNetwork(input_shape, initial_conv_filters=128, depth=[4, 8, 28, 3], filter_increment=[16, 32, 32, 128],\n                           cardinality=40, width=4, include_top=include_top, weights=weights, input_tensor=input_tensor,\n                           pooling=pooling, classes=classes)\n\n\ndef DPN107(input_shape=None,\n           include_top=True,\n           weights=None,\n           input_tensor=None,\n           pooling=None,\n           classes=13):\n    return DualPathNetwork(input_shape, initial_conv_filters=128, depth=[4, 8, 20, 3], filter_increment=[20, 64, 64, 128],\n                           cardinality=50, width=4, include_top=include_top, weights=weights, input_tensor=input_tensor,\n                           pooling=pooling, classes=classes)\n\n\ndef _initial_conv_block_inception(input, initial_conv_filters, weight_decay=5e-4):\n    ''' Adds an initial conv block, with batch norm and relu for the DPN\n    Args:\n        input: input tensor\n        initial_conv_filters: number of filters for initial conv block\n        weight_decay: weight decay factor\n    Returns: a keras tensor\n    '''\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    x = Conv2D(initial_conv_filters, (7, 7), padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(weight_decay), strides=(2, 2))(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation('relu')(x)\n\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n\n    return x\n\n\ndef _bn_relu_conv_block(input, filters, kernel=(3, 3), stride=(1, 1), weight_decay=5e-4):\n    ''' Adds a Batchnorm-Relu-Conv block for DPN\n    Args:\n        input: input tensor\n        filters: number of output filters\n        kernel: convolution kernel size\n        stride: stride of convolution\n    Returns: a keras tensor\n    '''\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    x = Conv2D(filters, kernel, padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(weight_decay), strides=stride)(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation('relu')(x)\n    return x\n\n\ndef _grouped_convolution_block(input, grouped_channels, cardinality, strides, weight_decay=5e-4):\n    ''' Adds a grouped convolution block. It is an equivalent block from the paper\n    Args:\n        input: input tensor\n        grouped_channels: grouped number of filters\n        cardinality: cardinality factor describing the number of groups\n        strides: performs strided convolution for downscaling if > 1\n        weight_decay: weight decay term\n    Returns: a keras tensor\n    '''\n    init = input\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    group_list = []\n\n    if cardinality == 1:\n        # with cardinality 1, it is a standard convolution\n        x = Conv2D(grouped_channels, (3, 3), padding='same', use_bias=False, strides=strides,\n                   kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(init)\n        x = BatchNormalization(axis=channel_axis)(x)\n        x = Activation('relu')(x)\n        return x\n\n    for c in range(cardinality):\n        x = Lambda(lambda z: z[:, :, :, c * grouped_channels:(c + 1) * grouped_channels]\n                   if K.image_data_format() == 'channels_last' else\n                   lambda z: z[:, c * grouped_channels:(c + 1) * grouped_channels, :, :])(input)\n\n        x = Conv2D(grouped_channels, (3, 3), padding='same', use_bias=False, strides=strides,\n                   kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n\n        group_list.append(x)\n\n    group_merge = concatenate(group_list, axis=channel_axis)\n    group_merge = BatchNormalization(axis=channel_axis)(group_merge)\n    group_merge = Activation('relu')(group_merge)\n    return group_merge\n\n\ndef _dual_path_block(input, pointwise_filters_a, grouped_conv_filters_b, pointwise_filters_c,\n                     filter_increment, cardinality, block_type='normal'):\n    '''\n    Creates a Dual Path Block. The first path is a ResNeXt type\n    grouped convolution block. The second is a DenseNet type dense\n    convolution block.\n\n    Args:\n        input: input tensor\n        pointwise_filters_a: number of filters for the bottleneck\n            pointwise convolution\n        grouped_conv_filters_b: number of filters for the grouped\n            convolution block\n        pointwise_filters_c: number of filters for the bottleneck\n            convolution block\n        filter_increment: number of filters that will be added\n        cardinality: cardinality factor\n        block_type: determines what action the block will perform\n            - `projection`: adds a projection connection\n            - `downsample`: downsamples the spatial resolution\n            - `normal`    : simple adds a dual path connection\n\n    Returns: a list of two output tensors - one path of ResNeXt\n        and another path for DenseNet\n\n    '''\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n    grouped_channels = int(grouped_conv_filters_b / cardinality)\n\n    init = concatenate(input, axis=channel_axis) if isinstance(input, list) else input\n\n    if block_type == 'projection':\n        stride = (1, 1)\n        projection = True\n    elif block_type == 'downsample':\n        stride = (2, 2)\n        projection = True\n    elif block_type == 'normal':\n        stride = (1, 1)\n        projection = False\n    else:\n        raise ValueError('`block_type` must be one of [\"projection\", \"downsample\", \"normal\"]. Given %s' % block_type)\n\n    if projection:\n        projection_path = _bn_relu_conv_block(init, filters=pointwise_filters_c + 2 * filter_increment,\n                                              kernel=(1, 1), stride=stride)\n        input_residual_path = Lambda(lambda z: z[:, :, :, :pointwise_filters_c]\n                                     if K.image_data_format() == 'channels_last' else\n                                     z[:, :pointwise_filters_c, :, :])(projection_path)\n        input_dense_path = Lambda(lambda z: z[:, :, :, pointwise_filters_c:]\n                                  if K.image_data_format() == 'channels_last' else\n                                  z[:, pointwise_filters_c:, :, :])(projection_path)\n    else:\n        input_residual_path = input[0]\n        input_dense_path = input[1]\n\n    x = _bn_relu_conv_block(init, filters=pointwise_filters_a, kernel=(1, 1))\n    x = _grouped_convolution_block(x, grouped_channels=grouped_channels, cardinality=cardinality, strides=stride)\n    x = _bn_relu_conv_block(x, filters=pointwise_filters_c + filter_increment, kernel=(1, 1))\n\n    output_residual_path = Lambda(lambda z: z[:, :, :, :pointwise_filters_c]\n                                  if K.image_data_format() == 'channels_last' else\n                                  z[:, :pointwise_filters_c, :, :])(x)\n    output_dense_path = Lambda(lambda z: z[:, :, :, pointwise_filters_c:]\n                               if K.image_data_format() == 'channels_last' else\n                               z[:, pointwise_filters_c:, :, :])(x)\n\n    residual_path = add([input_residual_path, output_residual_path])\n    dense_path = concatenate([input_dense_path, output_dense_path], axis=channel_axis)\n\n    return [residual_path, dense_path]\n\n\ndef _create_dpn(nb_classes, img_input, include_top, initial_conv_filters,\n                filter_increment, depth, cardinality=32, width=3, weight_decay=5e-4, pooling=None):\n    ''' Creates a ResNeXt model with specified parameters\n    Args:\n        initial_conv_filters: number of features for the initial convolution\n        include_top: Flag to include the last dense layer\n        initial_conv_filters: number of features for the initial convolution\n        filter_increment: number of filters incremented per block, defined as a list.\n            DPN-92  = [16, 32, 24, 128]\n            DON-98  = [16, 32, 32, 128]\n            DPN-131 = [16, 32, 32, 128]\n            DPN-107 = [20, 64, 64, 128]\n        depth: number or layers in the each block, defined as a list.\n            DPN-92  = [3, 4, 20, 3]\n            DPN-98  = [3, 6, 20, 3]\n            DPN-131 = [4, 8, 28, 3]\n            DPN-107 = [4, 8, 20, 3]\n        width: width multiplier for network\n        weight_decay: weight_decay (l2 norm)\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n            - `max-avg` means that both global average and global max\n                pooling will be applied to the output of the last\n                convolution layer\n    Returns: a Keras Model\n    '''\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n    N = list(depth)\n    base_filters = 256\n\n    # block 1 (initial conv block)\n    x = _initial_conv_block_inception(img_input, initial_conv_filters, weight_decay)\n\n    # block 2 (projection block)\n    filter_inc = filter_increment[0]\n    filters = int(cardinality * width)\n\n    x = _dual_path_block(x, pointwise_filters_a=filters,\n                         grouped_conv_filters_b=filters,\n                         pointwise_filters_c=base_filters,\n                         filter_increment=filter_inc,\n                         cardinality=cardinality,\n                         block_type='projection')\n\n    for i in range(N[0] - 1):\n        x = _dual_path_block(x, pointwise_filters_a=filters,\n                             grouped_conv_filters_b=filters,\n                             pointwise_filters_c=base_filters,\n                             filter_increment=filter_inc,\n                             cardinality=cardinality,\n                             block_type='normal')\n\n    # remaining blocks\n    for k in range(1, len(N)):\n        print(\"BLOCK %d\" % (k + 1))\n        filter_inc = filter_increment[k]\n        filters *= 2\n        base_filters *= 2\n\n        x = _dual_path_block(x, pointwise_filters_a=filters,\n                             grouped_conv_filters_b=filters,\n                             pointwise_filters_c=base_filters,\n                             filter_increment=filter_inc,\n                             cardinality=cardinality,\n                             block_type='downsample')\n\n        for i in range(N[k] - 1):\n            x = _dual_path_block(x, pointwise_filters_a=filters,\n                                 grouped_conv_filters_b=filters,\n                                 pointwise_filters_c=base_filters,\n                                 filter_increment=filter_inc,\n                                 cardinality=cardinality,\n                                 block_type='normal')\n\n    x = concatenate(x, axis=channel_axis)\n\n    if include_top:\n        avg = GlobalAveragePooling2D()(x)\n        max = GlobalMaxPooling2D()(x)\n        x = add([avg, max])\n        x = Lambda(lambda z: 0.5 * z)(x)\n        x = Dense(nb_classes, use_bias=False, kernel_regularizer=l2(weight_decay),\n                  kernel_initializer='he_normal', activation='softmax')(x)\n    else:\n        if pooling == 'avg':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == 'max':\n            x = GlobalMaxPooling2D()(x)\n        elif pooling == 'max-avg':\n            a = GlobalMaxPooling2D()(x)\n            b = GlobalAveragePooling2D()(x)\n            x = add([a, b])\n            x = Lambda(lambda z: 0.5 * z)(x)\n\n    return x\n\nif __name__ == '__main__':\n    model = DPN92((224, 224, 3))\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-17T15:31:15.347884Z","iopub.execute_input":"2024-03-17T15:31:15.348278Z","iopub.status.idle":"2024-03-17T15:31:27.665482Z","shell.execute_reply.started":"2024-03-17T15:31:15.348245Z","shell.execute_reply":"2024-03-17T15:31:27.664695Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"BLOCK 2\nBLOCK 3\nBLOCK 4\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n#Matplot Images\nimport matplotlib.image as mpimg\n# Tensflor and Keras Layer and Model and Optimize and Loss\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import *\nfrom tensorflow.keras.losses import BinaryCrossentropy\n#PreTrained Model\nfrom tensorflow.keras.applications import *\n#Image Generator DataAugmentation\n#Early Stopping\nfrom tensorflow.keras.callbacks import EarlyStopping\n# Warnings Remove \nimport warnings \nwarnings.filterwarnings(\"ignore\")\n#Splitting Data \n# import splitfolders\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nimport random\nfrom sklearn.cluster import KMeans\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T15:31:52.974547Z","iopub.execute_input":"2024-03-17T15:31:52.974938Z","iopub.status.idle":"2024-03-17T15:31:52.983798Z","shell.execute_reply.started":"2024-03-17T15:31:52.974907Z","shell.execute_reply":"2024-03-17T15:31:52.982706Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\ndirectory = \"/kaggle/input/mureda/training_set\"\n\nfilepath =[]\nlabel = []\n\nfolds = os.listdir(directory)\n\nfor fold in folds:\n    f_path = os.path.join(directory , fold)\n    \n    imgs = os.listdir(f_path)\n    \n    for img in imgs:\n        \n        img_path = os.path.join(f_path , img)\n        filepath.append(img_path)\n        label.append(fold)\n        \n#Concat data paths with labels\nfile_path_series = pd.Series(filepath , name= 'filepath')\nLabel_path_series = pd.Series(label , name = 'label')\ndf_train = pd.concat([file_path_series ,Label_path_series ] , axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T15:34:27.308377Z","iopub.execute_input":"2024-03-17T15:34:27.309356Z","iopub.status.idle":"2024-03-17T15:34:28.273689Z","shell.execute_reply.started":"2024-03-17T15:34:27.309320Z","shell.execute_reply":"2024-03-17T15:34:28.272601Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn. model_selection import train_test_split\ntrain_df , val_df = train_test_split(df_train ,train_size = 0.9 , shuffle = True ,random_state = 42 )","metadata":{"execution":{"iopub.status.busy":"2024-03-17T15:34:40.720054Z","iopub.execute_input":"2024-03-17T15:34:40.720696Z","iopub.status.idle":"2024-03-17T15:34:40.731581Z","shell.execute_reply.started":"2024-03-17T15:34:40.720667Z","shell.execute_reply":"2024-03-17T15:34:40.730708Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/paddydoctormedium'\n\nprint('Training Images:')\n# creating the training dataset\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset='training',\n    seed=123,\n    image_size=(224, 224),\n    batch_size=32)\n\n#Testing Augmented Data\nprint('Validation Images:')\nvalidation_ds = tf.keras.utils.image_dataset_from_directory(\n    data_dir, \n    validation_split=0.2,\n    subset='validation',\n    seed=123,\n    image_size=(224, 224),\n    batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T16:07:37.749463Z","iopub.execute_input":"2024-03-17T16:07:37.750369Z","iopub.status.idle":"2024-03-17T16:07:44.984308Z","shell.execute_reply.started":"2024-03-17T16:07:37.750333Z","shell.execute_reply":"2024-03-17T16:07:44.983564Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Training Images:\nFound 16225 files belonging to 13 classes.\nUsing 12980 files for training.\nValidation Images:\nFound 16225 files belonging to 13 classes.\nUsing 3245 files for validation.\n","output_type":"stream"}]},{"cell_type":"code","source":"for image_batch, labels_batch in train_ds:\n    print(\"Shape of X_train: \", image_batch.shape)\n    print(\"Shape of y_train: \", labels_batch.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-17T15:35:20.163053Z","iopub.execute_input":"2024-03-17T15:35:20.163687Z","iopub.status.idle":"2024-03-17T15:35:59.639137Z","shell.execute_reply.started":"2024-03-17T15:35:20.163657Z","shell.execute_reply":"2024-03-17T15:35:59.638068Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Shape of X_train:  (128, 224, 224, 3)\nShape of y_train:  (128,)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_ds = train_ds.map(lambda x, y: (x / 255.0, y))\n# Val Data\nvalidation_ds = validation_ds.map(lambda x, y: (x / 255.0, y))","metadata":{"execution":{"iopub.status.busy":"2024-03-17T15:36:10.873509Z","iopub.execute_input":"2024-03-17T15:36:10.873858Z","iopub.status.idle":"2024-03-17T15:36:10.927573Z","shell.execute_reply.started":"2024-03-17T15:36:10.873832Z","shell.execute_reply":"2024-03-17T15:36:10.926704Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\nmodel = DPN92(input_shape=(224, 224, 3))\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-17T15:36:27.215610Z","iopub.execute_input":"2024-03-17T15:36:27.216016Z","iopub.status.idle":"2024-03-17T15:36:39.151098Z","shell.execute_reply.started":"2024-03-17T15:36:27.215987Z","shell.execute_reply":"2024-03-17T15:36:39.150078Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"BLOCK 2\nBLOCK 3\nBLOCK 4\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping = EarlyStopping(\n    patience=10, \n    restore_best_weights=True,\n)\n\n# Train the model \nhistory = model.fit(train_ds,\n                    epochs=5,\n                    validation_data=validation_ds,\n                    callbacks=early_stopping)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T16:07:49.340538Z","iopub.execute_input":"2024-03-17T16:07:49.341178Z","iopub.status.idle":"2024-03-17T16:37:53.738134Z","shell.execute_reply.started":"2024-03-17T16:07:49.341148Z","shell.execute_reply":"2024-03-17T16:37:53.737085Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/5\n\u001b[1m405/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 622ms/step - accuracy: 0.0477 - loss: nan","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1710692318.258199     120 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0477 - loss: nan   ","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1710692371.842577     121 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m703s\u001b[0m 2s/step - accuracy: 0.0477 - loss: nan - val_accuracy: 0.0456 - val_loss: nan\nEpoch 2/5\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 668ms/step - accuracy: 0.0478 - loss: nan - val_accuracy: 0.0456 - val_loss: nan\nEpoch 3/5\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 679ms/step - accuracy: 0.0472 - loss: nan - val_accuracy: 0.0456 - val_loss: nan\nEpoch 4/5\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 678ms/step - accuracy: 0.0479 - loss: nan - val_accuracy: 0.0456 - val_loss: nan\nEpoch 5/5\n\u001b[1m406/406\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 683ms/step - accuracy: 0.0482 - loss: nan - val_accuracy: 0.0456 - val_loss: nan\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}